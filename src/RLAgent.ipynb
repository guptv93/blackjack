{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Agent\n",
    "Here we try to develop a RL Agent for learning the game of blackjack. We use Q-Learning for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Enlist Valid States**  \n",
    " \n",
    "The player state consists of three fields:\n",
    "1. Integer representing the sum of player card values\n",
    "2. Integer representing the value of dealer's first card\n",
    "3. Boolean representing whether the player has a usable Ace.\n",
    "\n",
    "First we write a function to get the current agent state. Then we enlist all the valid states that the player can be in. The player can continue playing (choosing between hit and state) in the valid states. We also assign an integer index to each state for easy reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_state(game):\n",
    "    return (game.get_sum(game.get_player_hand()), game.get_hand_value(game.get_dealer_hand())[0], game.hasAce)\n",
    "\n",
    "valid_states = [(x,y,z) for z in [True,False] for x in range(12,22) for y in range(2,12)]\n",
    "valid_states[160]\n",
    "\n",
    "def state_to_index(st):\n",
    "    if st in valid_states:\n",
    "        return valid_states.index(st)\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "state_to_index((20, 2, True))\n",
    "state_to_index((22, 2, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize Q-value Table** \n",
    "\n",
    "SARSA learning assigns a q-value to each state,action pair which represents the future rewards expected to be received if we take the given action in the given state. The actions that a player can take in any valid state are: hit and stand. We assign a q-value of 0 to each state, action pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hit': 0.0, 'stand': 0.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def zero_q_values():\n",
    "    qvals = {\"hit\": 0.0, \"stand\": 0.0}\n",
    "    return qvals\n",
    "\n",
    "q_value_table = [zero_q_values() for x in valid_states ]\n",
    "q_value_table[state_to_index((20, 2, True))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epsilon-Greedy Policy**  \n",
    "\n",
    "All control problems need a policy to decide what action to take in what state. We use epsilon-greedy policy, which tells us to take a random action epsilon number of times (exploration) and the best action according to current data 1-epsilon number of times (exploitation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stand\n",
      "stand\n",
      "stand\n",
      "stand\n",
      "hit\n",
      "hit\n",
      "stand\n",
      "stand\n",
      "stand\n",
      "hit\n"
     ]
    }
   ],
   "source": [
    "def epsilon_greedy(epsilon, q_values):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(list(q_values.keys()))\n",
    "    else:\n",
    "        if q_values[\"hit\"] > q_values[\"stand\"]:\n",
    "            return \"hit\"\n",
    "        elif q_values[\"hit\"] < q_values[\"stand\"]:\n",
    "            return \"stand\"\n",
    "        else:\n",
    "            return random.choice(list(q_values.keys()))\n",
    "        \n",
    "for i in range(10):\n",
    "    print(epsilon_greedy(0.1, zero_q_values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SARSA**  \n",
    "\n",
    "Finally we go on to learn the Q-value table. We use SARSA learning algorithm for the same. The intuition behind SARSA is very simple.\n",
    "\n",
    "We first randomly initialize q_value_table, with zero value for the terminal states. Then everytime we take action a in state s, we try to find the next state s' and next action (according to the same policy) a' and calculate the expected future reward. The expected future reward is given by:   \n",
    "immediate_reward + GAMMA * Q(s',a')  \n",
    "GAMMA is the discounting factor in the above equation.  \n",
    "\n",
    "The Q value of current state and action pair (s,a) is the moving average of all these expected future rewards. Thus we try to move the value of Q(s,a) towards the expected future reward by a constant averaging factor ALPHA.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 0.2\n",
    "ALPHA = 0.3\n",
    "GAMMA = 0.9\n",
    "#count_of_busts = 0\n",
    "#count_of_wins = 0\n",
    "\n",
    "for episode in range(500000):\n",
    "    game = BlackJack()\n",
    "    game.start_game()\n",
    "    state = state_to_index(get_state(game))\n",
    "    action = epsilon_greedy(EPSILON, q_value_table[state])\n",
    "    while(True):\n",
    "        if action==\"hit\":\n",
    "            game.hit()\n",
    "            next_state = state_to_index(get_state(game))\n",
    "        else:\n",
    "            game.stand()\n",
    "            next_state = -1\n",
    "        if next_state == -1:\n",
    "            q_value_table[state][action] += ALPHA*(game.result - q_value_table[state][action])\n",
    "            break\n",
    "        next_action = epsilon_greedy(EPSILON, q_value_table[next_state])\n",
    "        q_value_table[state][action] += ALPHA*( GAMMA*q_value_table[next_state][next_action] - q_value_table[state][action])\n",
    "        state = next_state\n",
    "        action = next_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Policy Table**\n",
    "\n",
    "Finally we try to develop the optimal policy from the Q value table develop. pi(s) = argmax Q(s,a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_table = {}\n",
    "value_function = {}\n",
    "for state in range(len(valid_states)):\n",
    "    if q_value_table[state][\"hit\"] > q_value_table[state][\"stand\"]:\n",
    "        policy_table[valid_states[state]] = \"hit\"\n",
    "        value_function[valid_states[state]] = q_value_table[state][\"hit\"]\n",
    "    else:\n",
    "        policy_table[valid_states[state]] = \"stand\"\n",
    "        value_function[valid_states[state]] = q_value_table[state][\"stand\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 2, False) : hit\n",
      "(12, 3, False) : hit\n",
      "(12, 4, False) : hit\n",
      "(12, 5, False) : hit\n",
      "(12, 6, False) : hit\n",
      "(12, 7, False) : hit\n",
      "(12, 8, False) : hit\n",
      "(12, 9, False) : hit\n",
      "(12, 10, False) : hit\n",
      "(12, 11, False) : hit\n",
      "(13, 2, False) : hit\n",
      "(13, 3, False) : hit\n",
      "(13, 4, False) : hit\n",
      "(13, 5, False) : hit\n",
      "(13, 6, False) : hit\n",
      "(13, 7, False) : hit\n",
      "(13, 8, False) : hit\n",
      "(13, 9, False) : hit\n",
      "(13, 10, False) : hit\n",
      "(13, 11, False) : hit\n",
      "(14, 2, False) : hit\n",
      "(14, 3, False) : stand\n",
      "(14, 4, False) : hit\n",
      "(14, 5, False) : hit\n",
      "(14, 6, False) : hit\n",
      "(14, 7, False) : hit\n",
      "(14, 8, False) : stand\n",
      "(14, 9, False) : hit\n",
      "(14, 10, False) : hit\n",
      "(14, 11, False) : hit\n",
      "(15, 2, False) : stand\n",
      "(15, 3, False) : stand\n",
      "(15, 4, False) : hit\n",
      "(15, 5, False) : hit\n",
      "(15, 6, False) : hit\n",
      "(15, 7, False) : hit\n",
      "(15, 8, False) : hit\n",
      "(15, 9, False) : hit\n",
      "(15, 10, False) : hit\n",
      "(15, 11, False) : stand\n",
      "(16, 2, False) : hit\n",
      "(16, 3, False) : hit\n",
      "(16, 4, False) : stand\n",
      "(16, 5, False) : stand\n",
      "(16, 6, False) : stand\n",
      "(16, 7, False) : stand\n",
      "(16, 8, False) : stand\n",
      "(16, 9, False) : hit\n",
      "(16, 10, False) : hit\n",
      "(16, 11, False) : hit\n",
      "(17, 2, False) : stand\n",
      "(17, 3, False) : stand\n",
      "(17, 4, False) : stand\n",
      "(17, 5, False) : hit\n",
      "(17, 6, False) : stand\n",
      "(17, 7, False) : stand\n",
      "(17, 8, False) : stand\n",
      "(17, 9, False) : stand\n",
      "(17, 10, False) : stand\n",
      "(17, 11, False) : stand\n",
      "(18, 2, False) : stand\n",
      "(18, 3, False) : stand\n",
      "(18, 4, False) : stand\n",
      "(18, 5, False) : stand\n",
      "(18, 6, False) : stand\n",
      "(18, 7, False) : stand\n",
      "(18, 8, False) : hit\n",
      "(18, 9, False) : hit\n",
      "(18, 10, False) : stand\n",
      "(18, 11, False) : hit\n",
      "(19, 2, False) : stand\n",
      "(19, 3, False) : stand\n",
      "(19, 4, False) : stand\n",
      "(19, 5, False) : stand\n",
      "(19, 6, False) : stand\n",
      "(19, 7, False) : stand\n",
      "(19, 8, False) : stand\n",
      "(19, 9, False) : stand\n",
      "(19, 10, False) : stand\n",
      "(19, 11, False) : stand\n",
      "(20, 2, False) : stand\n",
      "(20, 3, False) : stand\n",
      "(20, 4, False) : stand\n",
      "(20, 5, False) : stand\n",
      "(20, 6, False) : stand\n",
      "(20, 7, False) : stand\n",
      "(20, 8, False) : stand\n",
      "(20, 9, False) : stand\n",
      "(20, 10, False) : stand\n",
      "(20, 11, False) : stand\n",
      "(21, 2, False) : stand\n",
      "(21, 3, False) : stand\n",
      "(21, 4, False) : stand\n",
      "(21, 5, False) : stand\n",
      "(21, 6, False) : stand\n",
      "(21, 7, False) : stand\n",
      "(21, 8, False) : stand\n",
      "(21, 9, False) : stand\n",
      "(21, 10, False) : stand\n",
      "(21, 11, False) : stand\n"
     ]
    }
   ],
   "source": [
    "states_without_ace = [(x,y,False) for x in range(12,22) for y in range(2,12)]\n",
    "for state in states_without_ace:\n",
    "    print(str(state) + \" : \" + str(policy_table[state]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
