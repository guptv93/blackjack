{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heuristic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we first attempt to heuristically come up with probabilistic models of how humans take actions during blackjack. Then we use negative log-likelihoods to compare the heuristics and see which heuristic is mostly likely being followed by humans. We also compare these heuristic models with the optimal state-action function based model, and see which is more likely!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import *\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import colors\n",
    "from mpl_toolkits.mplot3d import Axes3D  \n",
    "import random\n",
    "from matplotlib.pyplot import figure\n",
    "from read_data import *\n",
    "import math\n",
    "from scipy.special import comb\n",
    "from scipy.optimize import fminbound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bootstrap code\n",
    "def get_state(game):\n",
    "    return (game.get_sum(game.get_player_hand()), game.get_hand_value(game.get_dealer_hand())[0], game.hasAce)\n",
    "\n",
    "valid_states = [(x,y,z) for z in [True,False] for x in range(12,22) for y in range(2,12)]\n",
    "states_without_ace = [(x,y,False) for x in range(12,22) for y in range(2,12)]\n",
    "states_with_ace = [(x,y,True) for x in range(12,22) for y in range(2,12)]\n",
    "\n",
    "def state_to_index(st):\n",
    "    if st in valid_states:\n",
    "        return valid_states.index(st)\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "state_to_index((20, 2, True))\n",
    "state_to_index((22, 2, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1\n",
    "#### P(win | state, action = hit_once)\n",
    "\n",
    "This model suggests that humans base their decision (to hit or not) on the probability of winning (player sum being less than or equal to 21 and dealer sum being lesser than player sum) if they hit once and then stop. Basically we assume $$P(stand) = P(win | state, action = hit)$$  \n",
    "\n",
    "We use monte-carlo sampling to calculate $P(stand) = P(win | state, action = hit)$. Optimally the player should base this decision on the possiblity of winning given he can hit multiple times, but we know that human mind doesn't always take decisions on a full-search exact-accuracy model. We also develop functions to compare our model with actual experiment data and measure the Maximum Likelihood Expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_likelihood(T, H, p):\n",
    "    p = p if p > 0.0 else 0.0+1e-10\n",
    "    p = p if p < 1.0 else 1.0-1e-10\n",
    "    result = math.log(comb(T, H)) + (H*math.log(p) + (T-H)*math.log(1.0-p))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES = 500\n",
    "def win_based_model(states, alpha):\n",
    "    wins = np.zeros(len(states), dtype = float)\n",
    "    for state in valid_states:\n",
    "        for i in range(SAMPLES):\n",
    "            ps, dc, ace = state\n",
    "            game = BlackJack()\n",
    "            game.set_state(ps, dc, ace)\n",
    "            if game.hit() == Status.PLAYER_BUST:\n",
    "                continue\n",
    "            if game.stand() == Status.PLAYER_WON:\n",
    "                wins[state_to_index(state)] += 1\n",
    "    wins = wins/SAMPLES\n",
    "    wins = (1-alpha)*wins + alpha*0.5\n",
    "    return wins\n",
    "\n",
    "def fit_win_based_model(alpha, human_data):\n",
    "    probabilities = win_based_model(valid_states, alpha)\n",
    "    total = 0\n",
    "    for index,row in human_data.iterrows():\n",
    "        total += compute_log_likelihood(row['Count'], row['HIT'], probabilities[state_to_index(row['State'])])\n",
    "    return -1*total\n",
    "    \n",
    "data = get_human_results()\n",
    "print(\"MLE for alpha = 1\" + str(fit_win_based_model(0.0, data)))\n",
    "minimizing_alpha = fminbound(fit_win_based_model, 0, 1, args=(data,))\n",
    "print(\"The best fitted value of alpha\" + str(minimizing_alpha))\n",
    "fit_win_based_model(minimizing_alpha, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5978495582456187"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1*compute_log_likelihood(20,15,0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2\n",
    "#### P(win | state, action = stand)\n",
    "\n",
    "This model suggests that humans base their decision (to stand or not) on the probability of winning if they stand in the current state. Basically we assume $$P(stand) = P(win | state, action = stand)$$ \n",
    "\n",
    "We use monte-carlo sampling to calculate $P(win | state, action = stand)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE for alpha=0 : 290.85705952508425\n",
      "The best fitted value of alpha0.2478019710168759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "228.9052224703976"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLES = 500\n",
    "def win_based_model(states, alpha):\n",
    "    wins = np.zeros(len(states), dtype = float)\n",
    "    losses = np.zeros(len(states), dtype = float)\n",
    "    for state in valid_states:\n",
    "        for i in range(SAMPLES):\n",
    "            ps, dc, ace = state\n",
    "            game = BlackJack()\n",
    "            game.set_state(ps, dc, ace)\n",
    "            if game.stand() == Status.PLAYER_WON:\n",
    "                wins[state_to_index(state)] += 1\n",
    "    wins = wins/SAMPLES\n",
    "    losses = 1 - wins\n",
    "    losses = (1-alpha)*losses + alpha*0.5\n",
    "    return losses\n",
    "\n",
    "def fit_win_based_model(alpha, human_data):\n",
    "    probabilities = win_based_model(valid_states, alpha)\n",
    "    total = 0\n",
    "    for index,row in human_data.iterrows():\n",
    "        total += compute_log_likelihood(row['Count'], row['HIT'], probabilities[state_to_index(row['State'])])\n",
    "    return -1*total\n",
    "    \n",
    "data = get_human_results()\n",
    "print(\"MLE for alpha=0: \" + str(fit_win_based_model(0.0, data)))\n",
    "minimizing_alpha = fminbound(fit_win_based_model, 0, 1, args=(data,))\n",
    "print(\"The best fitted value of alpha is \" + str(minimizing_alpha))\n",
    "fit_win_based_model(minimizing_alpha, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3\n",
    "#### Player Sum Based Model\n",
    "\n",
    "Here, we make a decision based only on the player's sum. He stands at a sum of 17 or more, and hits otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE for alpha=0: 331.9128048013683\n",
      "The best fitted value of alpha is 0.4860152273099868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "210.48221244715316"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum_based_model(states, alpha):\n",
    "    prob_hit = np.zeros(len(states), dtype = float)\n",
    "    for state in valid_states:\n",
    "        ps, dc, ace = state\n",
    "        if ps < 17:\n",
    "            prob_hit[state_to_index(state)] = 1\n",
    "        else:\n",
    "            prob_hit[state_to_index(state)] = 0\n",
    "    prob_hit = (1-alpha)*prob_hit + alpha*0.5\n",
    "    return prob_hit\n",
    "\n",
    "\n",
    "def fit_sum_based_model(alpha, human_data):\n",
    "    probabilities = sum_based_model(valid_states, alpha)\n",
    "    total = 0\n",
    "    for index,row in human_data.iterrows():\n",
    "        total += compute_log_likelihood(row['Count'], row['HIT'], probabilities[state_to_index(row['State'])])\n",
    "    return -1*total\n",
    "    \n",
    "data = get_human_results()\n",
    "print(\"MLE for alpha=0: \" + str(fit_sum_based_model(0.1, data)))\n",
    "minimizing_alpha = fminbound(fit_sum_based_model, 0, 1, args=(data,))\n",
    "print(\"The best fitted value of alpha is \" + str(minimizing_alpha))\n",
    "fit_sum_based_model(minimizing_alpha, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4\n",
    "#### Bust Based Policy\n",
    "\n",
    "Here, we consider what is the probability of getting bust if we hit. Based on that we decided whether to hit or not. Basically, $$P(hit) = P(nobust|state,hit)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE for alpha=0: 348.1108206519913\n",
      "The best fitted value of alpha is 0.570292789143172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "261.8607410931742"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bust_based_model(states, alpha):\n",
    "    prob_not_bust = np.zeros(len(states), dtype = float)\n",
    "    prob_hit = np.zeros(len(states), dtype = float)\n",
    "    for state in valid_states:\n",
    "        ps, dc, ace = state\n",
    "        prob_not_bust[state_to_index(state)] = (21 - ps)/13\n",
    "    prob_hit = (1-alpha)*prob_not_bust + alpha*0.5\n",
    "    return prob_hit\n",
    "\n",
    "def fit_bust_based_model(alpha, human_data):\n",
    "    probabilities = bust_based_model(valid_states, alpha)\n",
    "    total = 0\n",
    "    for index,row in human_data.iterrows():\n",
    "        total += compute_log_likelihood(row['Count'], row['HIT'], probabilities[state_to_index(row['State'])])\n",
    "    return -1*total\n",
    "    \n",
    "data = get_human_results()\n",
    "print(\"MLE for alpha=0: \" + str(fit_bust_based_model(0.1, data)))\n",
    "minimizing_alpha = fminbound(fit_bust_based_model, 0, 1, args=(data,))\n",
    "print(\"The best fitted value of alpha is \" + str(minimizing_alpha))\n",
    "fit_bust_based_model(minimizing_alpha, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
