{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heuristic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we first attempt to heuristically come up with probabilistic models of how humans take actions during blackjack. Then we use negative log-likelihoods to compare the heuristics and see which heuristic is mostly likely being followed by humans. We also compare these heuristic models with the optimal state-action function based model, and see which is more likely!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import *\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import colors\n",
    "from mpl_toolkits.mplot3d import Axes3D  \n",
    "import random\n",
    "from matplotlib.pyplot import figure\n",
    "from read_data import *\n",
    "import math\n",
    "from scipy.special import comb\n",
    "from scipy.optimize import fminbound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bootstrap code\n",
    "def get_state(game):\n",
    "    return (game.get_sum(game.get_player_hand()), game.get_hand_value(game.get_dealer_hand())[0], game.hasAce)\n",
    "\n",
    "valid_states = [(x,y,z) for z in [True,False] for x in range(12,22) for y in range(2,12)]\n",
    "states_without_ace = [(x,y,False) for x in range(12,22) for y in range(2,12)]\n",
    "states_with_ace = [(x,y,True) for x in range(12,22) for y in range(2,12)]\n",
    "\n",
    "def state_to_index(st):\n",
    "    if st in valid_states:\n",
    "        return valid_states.index(st)\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "state_to_index((20, 2, True))\n",
    "state_to_index((22, 2, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1\n",
    "#### P(Winning | Current State, Hit Once)\n",
    "\n",
    "This model suggests that humans base their decision (to hit or not) on the probability of winning (player sum being less than or equal to 21 and dealer sum being lesser than player sum) if they hit once and then stop. We use monte-carlo sampling to calculate P(winning | current_state, hit_once). Optimally the player should base this decision on the possiblity of winning given he can hit multiple times, but we know that human mind doesn't always take decisions on a full-width exact accuracy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES = 500\n",
    "def winning_based_model(states, alpha):\n",
    "    wins = np.zeros(len(states), dtype = float)\n",
    "    for state in valid_states:\n",
    "        for i in range(SAMPLES):\n",
    "            ps, dc, ace = state\n",
    "            game = BlackJack()\n",
    "            game.set_state(ps, dc, ace)\n",
    "            if game.hit() == Status.PLAYER_BUST:\n",
    "                continue\n",
    "            if game.stand() == Status.PLAYER_WON:\n",
    "                wins[state_to_index(state)] += 1\n",
    "    wins = wins/SAMPLES\n",
    "    wins = (1-alpha)*wins + alpha*0.5\n",
    "    return wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288.40595221233787"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_log_likelihood(T, H, p):\n",
    "    p = p if p > 0.0 else 0.0+1e-10\n",
    "    p = p if p < 1.0 else 1.0-1e-10\n",
    "    result = math.log(comb(T, H)) + (H*math.log(p) + (T-H)*math.log(1.0-p))\n",
    "    return result\n",
    "    \n",
    "def fit_winning_based_model(alpha, human_data):\n",
    "    probabilities = winning_based_model(valid_states, alpha)\n",
    "    total = 0\n",
    "    for index,row in human_data.iterrows():\n",
    "        total += compute_log_likelihood(row['Count'], row['HIT'], probabilities[state_to_index(row['State'])])\n",
    "    return -1*total\n",
    "    \n",
    "data = get_human_results()\n",
    "fit_winning_based_model(0.1, data)\n",
    "minimizing_alpha = fminbound(fit_winning_based_model, 0, 1, args=(data,))\n",
    "fit_winning_based_model(minimizing_alpha, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8923681894742407"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimizing_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.444479865707077"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1*compute_log_likelihood(20,15,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
